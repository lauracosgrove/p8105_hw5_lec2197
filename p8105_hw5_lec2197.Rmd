---
title: "Homework 5"
author: "Laura Cosgrove"
date: "11/3/2018"
output: github_document
---

## Problem 1

```{r packages}
library(tidyverse)
```

First, I'll create  a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time, by reading in data from multiple files.

```{r load and tidy study data, message=FALSE}

directory_base <- "./data/"
study_data <- tibble(files = str_c(directory_base, list.files("./data")))

#Reading data from multipe files and clean observation names

study_data <- study_data %>% 
  mutate(subject_data = map(study_data$files, read_csv)) %>% 
  unnest() %>% 
  mutate(files = str_replace(files, "./data/", "")) %>% 
  mutate(files = str_replace(files, ".csv", "")) %>% 
  mutate(group = str_detect(files, "con")) %>% 
  mutate(group = ifelse(group == TRUE, "control", "experimental")) %>% 
  mutate(files = as.factor(str_replace(files, "^..._", ""))) %>% 
  rename(subject_ID = files) %>%
  gather(key = week, value = value, starts_with("week")) %>% 
  mutate(week = as.numeric(str_replace(week, "week_", "")))

study_data %>% 
  ggplot(aes(x = week, y = value, color = subject_ID)) +
  geom_line() +
  facet_grid(~group) +
  labs(
    title = "Control and Experimental Change Over Time"
  )
```

The experimental group shows a general trend of increase over time, while the control group's trend is largely static. This dataset looks promising for further hypothesis testing analyses.

## Problem 2

The Washington Post gathered homicide data on large U.S. cities, and made the data public as part of their investigation. First, I'll load in the raw data and look at the years reported it covers as well as the outcomes it studies.

```{r}
homicide_data <- read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")

homicide_data %>% 
  mutate(year_reported = str_sub(reported_date, 1, 4)) %>% 
  distinct(year_reported) %>% 
  arrange(as.numeric(year_reported))

homicide_data %>% 
  distinct(disposition)

```

This dataset contains information about `r nrow(homicide_data)` homicides in `r nrow(homicide_data %>% distinct(city))` U.S. cities in `r nrow(homicide_data %>% distinct(state))` states reported from 2007 to 2017. The victim's name, rage, age, and sex are reported for each homicide, as well as the status of the case (closed without arrest, closed by arrest, or open).

Next, I'll create a variable to look at the summary data within cities: the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r proportion total vs unsolved dataframe}
prop_homicides <- homicide_data %>% 
  mutate(city_state = str_c(city, state, sep = ", ")) %>%
  select(city_state, disposition) %>% 
  group_by(city_state) %>%
  mutate(unsolved = ifelse(disposition == "Open/No arrest", 1, 0)) %>% 
  summarize(total = n(),
            unsolved = sum(unsolved))

prop_homicides %>% 
  knitr::kable()
```

Tulsa, AL seems to have some strange data: This might be a miscoding. I will explore:

```{r explore weird tulsa data}
prop_homicides %>% 
  filter(city_state == "Tulsa, AL")

homicide_data %>% 
  filter(state == "AL" & city == "Tulsa")
```

Since there's only one entry, it's probably miscoded either for Tulsa, OK or Birmingham, AL. I'm not sure which, so I'll remove it for the final analyses below:

```{r remove tulsa al}
prop_homicides <- prop_homicides %>% 
  filter(city_state != "Tulsa, AL")
```

### Exploring Estimates for Proportions of Unsolved Murders

For Baltimore, MD, I will  estimate the proportion of homicides that are unsolved using `prop.test`, save the output of prop.test as an R object, and apply `broom::tidy` to this object in order to pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r baltimore}
baltimore_data <- prop_homicides %>% 
  filter(city_state == "Baltimore, MD")

baltimore_proptest <- prop.test(baltimore_data$unsolved, baltimore_data$total) 

baltimore_proptest <- broom::tidy(baltimore_proptest)

baltimore_proptest %>% 
  pull(estimate)
baltimore_proptest %>% 
  pull(conf.low)
baltimore_proptest %>% 
  pull(conf.high)

```

Using this method as a framework, I'll create a function to run `prop.test` for each of the cities in my dataset, and I will extract both the proportion of unsolved homicides and the confidence interval for each and save that in the previous `prop_homicides` dataframe.

```{r}
prop_homicides <- nest(prop_homicides, total:unsolved) 

prop_test_homicides = function(df) {
  prop.test(x = df$unsolved, n = df$total)
}

prop_homicides <- prop_homicides %>% 
  mutate(prop_test = map(data, prop_test_homicides)) %>% 
  select(city_state, prop_test) %>% 
  mutate(prop_test = map(prop_test, broom::tidy)) %>% 
  unnest() %>% 
  select(city_state, estimate, conf.low, conf.high)
```


Finally, I'll use a scatter plot with error bars to visualize the differences by city in the point estimates of the proportion and confidence intervals given by the proportion test.  

```{r} 
prop_homicides %>% 
  mutate(city_state = factor(city_state)) %>% 
  mutate(city_state = forcats::fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  ggthemes::theme_fivethirtyeight() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = rel(0.8))) +
  geom_hline(yintercept = 0.50, color = "blue", size = 0.2) +
  labs(title = "Proportion of Unsolved Homicides in Major US Cities",
       caption = "Data from Washington Post")
```



